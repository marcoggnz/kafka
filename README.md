# Pipeline de AnÃ¡lisis de Sentimientos en Twitter con Kafka

Un sistema de anÃ¡lisis de sentimientos en tiempo real utilizando **Apache Kafka** y **Hugging Face Transformers**, impulsado por un clÃºster local de Kafka en Docker. Los tweets se simulan desde un archivo CSV y se analizan con un modelo BERT ajustado. Este proyecto forma parte del MÃ¡ster en Big Data Architecture & Engineering en Datahack.

## TecnologÃ­as Utilizadas

- **Python 3.12**
- **Apache Kafka** (mediante Docker Compose)
- **Kafka UI** para monitorizaciÃ³n de topics
- **pandas** para el manejo de datos
- **transformers** de Hugging Face para NLP
- **PyTorch** (via `transformers[torch]`) como backend de ML
- **Docker Compose** para el clÃºster de Kafka
- **Matplotlib** para visualizaciones bÃ¡sicas
- **Kafka-Python** para producir/consumir mensajes

## IntroducciÃ³n

Este proyecto simula un flujo de Twitter en tiempo real mediante:
1. Lectura de mensajes desde un dataset (`test.csv`).
2. ProducciÃ³n de los mensajes a un tÃ³pico de Kafka (`x-data`).
3. Consumo de los mensajes usando un consumidor de Kafka.
4. AnÃ¡lisis de sentimientos de cada mensaje.
5. Guardado de resultados en un archivo CSV.
6. VisualizaciÃ³n de la distribuciÃ³n de sentimientos.

Es ideal para aprender a integrar streaming de datos con pipelines de machine learning en Python.

## Instrucciones de configuraciÃ³n

### Requisitos previos

* Python 3.12 instalado
* Docker y Docker Compose instalados
* (Solo WSL) AÃ±adir esto en ```/etc/hosts```:
```
127.0.0.1 kafka1 kafka2 kafka3
```

### OpciÃ³n 1: Despliegue con un clic (Recomendado para desarrollo local)

1. Clonar el repositorio
```
git clone https://github.com/marcoggnz/datahack-kafka.git
cd datahack-kafka
```

2. Abrir una terminal y hacer el script ejecutable:

```
chmod +x run_pipeline.sh
```


3. Ejecutar toda la app:

```
./run_pipeline.sh
```

El script harÃ¡:
* Iniciar el clÃºster de Kafka vÃ­a Docker.
* Activar el entorno virtual de Python.
* Instalar las dependencias fijadas.
* Enviar tweets mediante el productor.
* Analizar esos tweets con el consumidor.
* Generar un grÃ¡fico de barras de sentimientos.

### OpciÃ³n 2: ConfiguraciÃ³n manual
1. Clonar el repositorio
```
git clone https://github.com/marcoggnz/datahack-kafka.git
cd datahack-kafka
```

2. Iniciar el clÃºster de Kafka  
Iniciar el clÃºster local de Kafka + Zookeeper y Kafka UI usando Docker Compose:

```
docker compose -f images/docker-compose-cluster-kafka.yml up -d
```


3. Crear y activar un entorno virtual
```
python3 -m venv .venv
source .venv/bin/activate
```

4. Instalar dependencias de Python
```
pip install kafka-python==2.1.5 \
            transformers[torch]==4.51.0 \
            pandas==2.2.3 \
            matplotlib==3.8.4

```
O simplemente ejecutar:
```
pip install -r requirements.txt
```

5. Ejecutar el Productor
```
python src/producer.py
```

Esto enviarÃ¡ un lote de tweets simulados a Kafka.

6. Ejecutar el Consumidor (en una nueva terminal)
```
python src/consumer.py
```

ProcesarÃ¡ los mensajes y guardarÃ¡ los resultados en **sentiment_results.csv**.

7. Visualizar los Resultados
```
python src/visualization.py
```

Esto generarÃ¡ **sentiment_distribution.png** mostrando el desglose de sentimientos.

8. Paso opcional de limpieza:

```
docker compose -f images/docker-compose-cluster-kafka.yml down
```

## MonitorizaciÃ³n

Abrir Kafka UI en http://localhost:8080

## Estructura del Proyecto

```
â”œâ”€â”€ data/
â”‚   â””â”€â”€ test.csv
â”œâ”€â”€ images/
â”‚   â”œâ”€â”€ docker-compose-cluster-kafka.yml
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ producer.py
â”‚   â”œâ”€â”€ consumer.py
â”‚   â””â”€â”€ visualization.py
â”œâ”€â”€ run_pipeline.sh           # Script de inicio con un clic ðŸŸ¢
â”œâ”€â”€ sentiment_results.csv     # Archivo de salida con predicciones
â”œâ”€â”€ sentiment_distribution.png # GrÃ¡fico de resultados de sentimiento
â”œâ”€â”€ producer.log              # Archivo de log
â”œâ”€â”€ consumer.log              # Archivo de log
â”œâ”€â”€ README.md                 # EstÃ¡s aquÃ­
â””â”€â”€ .venv/                    # Entorno virtual (no se sube al repositorio)
```

## Notas

* El nombre del tÃ³pico, el tamaÃ±o de muestra y el modelo son configurables en config.py.
* Los logs se registran tanto en terminal como en archivos.
* DiseÃ±ado para desarrollo local, pero extensible para uso en la nube.

## Ideas de mejora

Hay muchas formas en las que este proyecto puede mejorar, ya que el objetivo principal es familiarizarse con el entorno y flujo de Kafka. Algunas posibles mejoras son:
* Construir un dashboard (por ejemplo, con Streamlit o Dash).
* Enviar resultados a una base de datos o API.
* Integrarse con plataformas Kafka gestionadas (como Confluent Cloud).